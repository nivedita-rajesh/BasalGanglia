{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RInnvagrvJ_4",
        "outputId": "4b130760-085e-42e1-fbbf-feb12a5810fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Episode 1:\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 1, Total Reward: 0.1\n",
            "\n",
            "Episode 2:\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 2, Total Reward: 0.1\n",
            "\n",
            "Episode 3:\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 3, Total Reward: 0.1\n",
            "\n",
            "Episode 4:\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 4, Total Reward: 0.1\n",
            "\n",
            "Episode 5:\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 5, Total Reward: 0.1\n",
            "\n",
            "Episode 6:\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 6, Total Reward: 0.1\n",
            "\n",
            "Episode 7:\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 7, Total Reward: 0.1\n",
            "\n",
            "Episode 8:\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Transition: Striatum -> GPe, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: GPe -> STN, Reward: 0\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Transition: STN -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 8, Total Reward: 0.1\n",
            "\n",
            "Episode 9:\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Transition: Striatum -> GPe, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: GPe -> STN, Reward: 0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Transition: STN -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 9, Total Reward: 0.1\n",
            "\n",
            "Episode 10:\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Transition: Striatum -> GPe, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: GPe -> STN, Reward: 0\n",
            "Transition: STN -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 10, Total Reward: 0.1\n",
            "\n",
            "Episode 11:\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "Transition: Striatum -> GPe, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: GPe -> STN, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: STN -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 11, Total Reward: 0.1\n",
            "\n",
            "Episode 12:\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Transition: Striatum -> GPe, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: GPe -> STN, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: STN -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 12, Total Reward: 0.1\n",
            "\n",
            "Episode 13:\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 13, Total Reward: 0.1\n",
            "\n",
            "Episode 14:\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Transition: Striatum -> GPe, Reward: 0\n",
            "Transition: GPe -> STN, Reward: 0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Transition: STN -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 14, Total Reward: 0.1\n",
            "\n",
            "Episode 15:\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "Transition: Striatum -> GPe, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: GPe -> STN, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: STN -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 15, Total Reward: 0.1\n",
            "\n",
            "Episode 16:\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 16, Total Reward: 0.1\n",
            "\n",
            "Episode 17:\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 17, Total Reward: 0.1\n",
            "\n",
            "Episode 18:\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "Transition: Striatum -> GPe, Reward: 0\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "Transition: GPe -> STN, Reward: 0\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Transition: STN -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 18, Total Reward: 0.1\n",
            "\n",
            "Episode 19:\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Transition: Striatum -> GPe, Reward: 0\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Transition: GPe -> STN, Reward: 0\n",
            "Transition: STN -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 19, Total Reward: 0.1\n",
            "\n",
            "Episode 20:\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Transition: Striatum -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 20, Total Reward: 0.1\n",
            "\n",
            "Episode 21:\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Transition: Striatum -> GPe, Reward: 0\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Transition: GPe -> STN, Reward: 0\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Transition: STN -> GPi, Reward: 0\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Transition: GPi -> Thalamus, Reward: 0.1\n",
            "Episode: 21, Total Reward: 0.1\n",
            "\n",
            "Episode 22:\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "Transition: Cortex -> Striatum, Reward: 0\n",
            "1/1 [==============================] - ETA: 0s"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"1\"\n",
        "\n",
        "class BasalGangliaMDP(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define states\n",
        "        self.states = ['Cortex', 'Striatum', 'GPe', 'STN', 'GPi', 'Thalamus']\n",
        "\n",
        "        # Define actions\n",
        "        self.actions = ['Change State']\n",
        "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
        "\n",
        "        # Define transition probabilities (consult experts for accurate values)\n",
        "        self.transition_probs = {\n",
        "            'Cortex': {\n",
        "                'Change State': {'Striatum': 1.0}\n",
        "            },\n",
        "            'Striatum': {\n",
        "                'Change State': {'GPe': 0.5, 'GPi': 0.5}\n",
        "            },\n",
        "            'GPe': {\n",
        "                'Change State': {'STN': 1.0}\n",
        "            },\n",
        "            'STN': {\n",
        "                'Change State': {'GPi': 1.0}\n",
        "            },\n",
        "            'GPi': {\n",
        "                'Change State': {'Thalamus': 1.0}\n",
        "            },\n",
        "            'Thalamus': {\n",
        "                'Change State': {'Thalamus': 1.0}  # Terminal state\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Define rewards (consult experts for appropriate values)\n",
        "        self.rewards = {\n",
        "            ('Cortex', 'Change State', 'Striatum'): 0,  # Neutral transition\n",
        "            ('Striatum', 'Change State', 'GPe'): 1,  # Mild penalty for indirect pathway\n",
        "            ('Striatum', 'Change State', 'GPi'): 1,  # Reward for direct pathway\n",
        "            ('GPe', 'Change State', 'STN'): 0.5,  # Mild penalty for prolonged indirect pathway\n",
        "            ('STN', 'Change State', 'GPi'): 0.5,  # Partial reward for returning to GPi\n",
        "            ('GPi', 'Change State', 'Thalamus'): 2,  # Significant reward for reaching Thalamus\n",
        "            ('Thalamus', 'Change State', 'Thalamus'): 0.1,  # Small reward for staying in Thalamus\n",
        "        }\n",
        "\n",
        "\n",
        "        self.state = 'Cortex'  # Initial state\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        next_state_probs = self.transition_probs[self.state][action]\n",
        "        next_state = np.random.choice(list(next_state_probs.keys()), p=list(next_state_probs.values()))\n",
        "\n",
        "        self.state = next_state\n",
        "\n",
        "        reward = self.rewards.get((self.state, action, next_state), 0)\n",
        "        done = next_state == 'Thalamus'  # Terminal state\n",
        "        info = {}\n",
        "\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 'Cortex'\n",
        "        return self.state\n",
        "\n",
        "    def render(self):\n",
        "        print(self.state)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_space_size, action_space_size, learning_rate=0.001, discount_factor=0.9, exploration_prob=0.1):\n",
        "        self.state_space_size = state_space_size\n",
        "        self.action_space_size = action_space_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_prob = exploration_prob\n",
        "\n",
        "        # Build Q-network\n",
        "        self.q_network = self.build_q_network()\n",
        "\n",
        "        # Target Q-network (for stability)\n",
        "        self.target_q_network = self.build_q_network()\n",
        "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "        # Experience replay buffer\n",
        "        self.memory = []\n",
        "\n",
        "    def build_q_network(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_space_size,), dtype=tf.float32),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_space_size)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\n",
        "\n",
        "    def select_action(self, state):\n",
        "      if np.random.rand() < self.exploration_prob:\n",
        "          return np.random.choice(self.action_space_size)\n",
        "      else:\n",
        "          # Convert state index to one-hot encoding\n",
        "          state_one_hot = np.zeros(self.state_space_size)\n",
        "          state_one_hot[state] = 1\n",
        "\n",
        "          # Reshape state for model prediction\n",
        "          state_one_hot = state_one_hot.reshape(1, -1)  # Reshape to (1, state_space_size)\n",
        "\n",
        "          # Predict Q-values for the current state\n",
        "          q_values = self.q_network.predict(state_one_hot)\n",
        "\n",
        "          # Select action with the highest Q-value\n",
        "          return np.argmax(q_values[0])\n",
        "\n",
        "    def update_q_network(self, batch_size, states=None):\n",
        "      if len(self.memory) < batch_size:\n",
        "          return\n",
        "\n",
        "      if states is None:\n",
        "          # Sample a batch from memory\n",
        "          samples = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "          batch = [self.memory[i] for i in samples]\n",
        "\n",
        "          # Extract components from the batch\n",
        "          states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "          # Convert state indices to one-hot encoding\n",
        "          states = np.eye(len(env.states), dtype=int)[np.array(states)]\n",
        "\n",
        "          q_values = self.q_network.predict(states)\n",
        "          next_q_values = self.target_q_network.predict(states)\n",
        "\n",
        "          # Update Q-values based on Bellman equation\n",
        "          for i in range(batch_size):\n",
        "              target = rewards[i] + self.discount_factor * np.max(next_q_values[i]) * (1 - dones[i])\n",
        "              q_values[i, actions[i]] = target\n",
        "\n",
        "          # Train the Q-network\n",
        "          self.q_network.fit(states, q_values, verbose=0)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
        "\n",
        "# Instantiate the environment and the DQN agent\n",
        "env = BasalGangliaMDP()\n",
        "agent = DQNAgent(state_space_size=len(env.states), action_space_size=env.action_space.n)\n",
        "\n",
        "# Training the DQN agent\n",
        "num_episodes = 100\n",
        "batch_size = 32\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state_episode = []\n",
        "    total_reward = 0\n",
        "\n",
        "    state = env.reset()\n",
        "    state_episode.append(state)\n",
        "\n",
        "    print(f\"\\nEpisode {episode + 1}:\")\n",
        "\n",
        "    while True:\n",
        "        # Convert state to its corresponding index\n",
        "        state_index = env.states.index(state)\n",
        "\n",
        "        action = agent.select_action(state_index)\n",
        "        next_state, reward, done, _ = env.step(env.actions[action])\n",
        "\n",
        "        print(f\"Transition: {env.states[state_index]} -> {next_state}, Reward: {reward}\")\n",
        "\n",
        "\n",
        "        # Convert next_state to its corresponding index\n",
        "        next_state_index = env.states.index(next_state)\n",
        "\n",
        "        # Store experience in replay buffer\n",
        "        agent.memory.append((state_index, action, reward, next_state_index, done))\n",
        "\n",
        "        # Update Q-network\n",
        "        if len(state_episode) >= 2:\n",
        "            agent.update_q_network(batch_size, states=state_episode)\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        # Add more states to the state_episode list\n",
        "        if len(state_episode) < 10:\n",
        "            state_episode.append(state)\n",
        "        else:\n",
        "            # Remove the first state in the state_episode list\n",
        "            state_episode.pop(0)\n",
        "            # Add the current state to the state_episode list\n",
        "            state_episode.append(state)\n",
        "\n",
        "    # Update target network periodically\n",
        "    if episode % 10 == 0:\n",
        "        agent.update_target_network()\n",
        "\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Testing the trained DQN agent\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "print(\"\\nFinally chosen pathway: \")\n",
        "while not done:\n",
        "    # Convert state to its corresponding index\n",
        "    state_index = env.states.index(state)\n",
        "\n",
        "    action = agent.select_action(state_index)\n",
        "    next_state, reward, done, _ = env.step(env.actions[action])\n",
        "\n",
        "    print(f\"Transition: {env.states[state_index]} -> {next_state}, Reward: {reward}\")\n",
        "    state = next_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDdr-uxIvP2P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}